
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-02-25 04:14:09.228315: Using torch.compile... 
2025-02-25 04:14:24.090154: do_dummy_2d_data_aug: False 
2025-02-25 04:14:54.436913: Using splits from existing split file: /content/drive/MyDrive/nnUNet/nnUNet_preprocessed/Dataset001_BoneScanAnterior/splits_final.json 
2025-02-25 04:14:55.138467: The split file contains 1 splits. 
2025-02-25 04:14:55.141846: Desired fold for training: 0 
2025-02-25 04:14:55.144267: This split has 1658 training and 90 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 48, 'patch_size': [512, 128], 'median_image_size_in_voxels': [512.0, 128.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_BoneScanAnterior', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 512, 128], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 220.8397674560547, 'median': 229.0, 'min': 0.0, 'percentile_00_5': 88.0, 'percentile_99_5': 255.0, 'std': 30.951431274414062}}} 
 
2025-02-25 04:19:15.896529: unpacking dataset... 
2025-02-25 04:21:33.571376: unpacking done... 
2025-02-25 04:21:36.147157: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-02-25 04:21:36.789155:  
2025-02-25 04:21:36.792802: Epoch 50 
2025-02-25 04:21:36.795889: Current learning rate: 0.00955 
2025-02-25 04:30:50.769103: train_loss -0.8271 
2025-02-25 04:30:50.778378: val_loss -0.5928 
2025-02-25 04:30:50.783450: Pseudo dice [0.9513, 0.7578, 0.5514, 0.8851, 0.8066, 0.6976, 0.7562, 0.8516, 0.8299, 0.8201, 0.8942, 0.8722] 
2025-02-25 04:30:50.790400: Epoch time: 553.98 s 
2025-02-25 04:30:50.795631: Yayy! New best EMA pseudo Dice: 0.8029 
2025-02-25 04:31:08.024483:  
2025-02-25 04:31:08.031015: Epoch 51 
2025-02-25 04:31:08.035654: Current learning rate: 0.00954 
2025-02-25 04:37:48.935485: train_loss -0.8305 
2025-02-25 04:37:48.943153: val_loss -0.5748 
2025-02-25 04:37:48.949956: Pseudo dice [0.9501, 0.7501, 0.5536, 0.8814, 0.8025, 0.6871, 0.7512, 0.8473, 0.8253, 0.8131, 0.8956, 0.8693] 
2025-02-25 04:37:48.956838: Epoch time: 400.91 s 
2025-02-25 04:37:51.513631:  
2025-02-25 04:37:51.520316: Epoch 52 
2025-02-25 04:37:51.527170: Current learning rate: 0.00953 
2025-02-25 04:44:32.690513: train_loss -0.8284 
2025-02-25 04:44:32.698893: val_loss -0.5819 
2025-02-25 04:44:32.705381: Pseudo dice [0.9517, 0.7549, 0.5449, 0.8809, 0.8018, 0.6948, 0.7584, 0.8521, 0.8265, 0.8145, 0.8948, 0.8726] 
2025-02-25 04:44:32.711894: Epoch time: 401.18 s 
2025-02-25 04:44:32.718707: Yayy! New best EMA pseudo Dice: 0.803 
2025-02-25 04:44:37.672939:  
2025-02-25 04:44:37.687191: Epoch 53 
2025-02-25 04:44:37.695060: Current learning rate: 0.00952 
2025-02-25 04:51:24.259662: train_loss -0.8302 
2025-02-25 04:51:24.266208: val_loss -0.5866 
2025-02-25 04:51:24.272489: Pseudo dice [0.9495, 0.7502, 0.5739, 0.883, 0.8121, 0.6925, 0.7473, 0.8523, 0.8279, 0.813, 0.8954, 0.8747] 
2025-02-25 04:51:24.277997: Epoch time: 406.59 s 
2025-02-25 04:51:24.284487: Yayy! New best EMA pseudo Dice: 0.8033 
2025-02-25 04:51:29.104443:  
2025-02-25 04:51:29.122131: Epoch 54 
2025-02-25 04:51:29.125755: Current learning rate: 0.00951 
2025-02-25 04:58:15.058890: train_loss -0.8318 
2025-02-25 04:58:15.067173: val_loss -0.5853 
2025-02-25 04:58:15.074996: Pseudo dice [0.9503, 0.7562, 0.5795, 0.8821, 0.8061, 0.6837, 0.7511, 0.8511, 0.8255, 0.81, 0.8956, 0.8754] 
2025-02-25 04:58:15.082667: Epoch time: 405.96 s 
2025-02-25 04:58:15.090127: Yayy! New best EMA pseudo Dice: 0.8035 
2025-02-25 04:58:20.913617:  
2025-02-25 04:58:20.919099: Epoch 55 
2025-02-25 04:58:20.925370: Current learning rate: 0.0095 
2025-02-25 05:04:54.947042: train_loss -0.8329 
2025-02-25 05:04:54.953052: val_loss -0.5793 
2025-02-25 05:04:54.958915: Pseudo dice [0.9488, 0.7587, 0.5711, 0.8855, 0.8111, 0.6913, 0.7435, 0.8492, 0.8321, 0.8154, 0.8924, 0.8685] 
2025-02-25 05:04:54.963800: Epoch time: 394.04 s 
2025-02-25 05:04:54.969296: Yayy! New best EMA pseudo Dice: 0.8037 
2025-02-25 05:04:59.643487:  
2025-02-25 05:04:59.649855: Epoch 56 
2025-02-25 05:04:59.654907: Current learning rate: 0.00949 
2025-02-25 05:11:36.844716: train_loss -0.8336 
2025-02-25 05:11:36.863934: val_loss -0.575 
2025-02-25 05:11:36.872566: Pseudo dice [0.9492, 0.7559, 0.5706, 0.8827, 0.8016, 0.685, 0.751, 0.8515, 0.832, 0.8103, 0.8941, 0.8717] 
2025-02-25 05:11:36.880529: Epoch time: 397.2 s 
2025-02-25 05:11:36.892433: Yayy! New best EMA pseudo Dice: 0.8038 
2025-02-25 05:11:41.188099:  
2025-02-25 05:11:41.195456: Epoch 57 
2025-02-25 05:11:41.200063: Current learning rate: 0.00949 
2025-02-25 05:18:13.121882: train_loss -0.835 
2025-02-25 05:18:13.127855: val_loss -0.5736 
2025-02-25 05:18:13.132972: Pseudo dice [0.9491, 0.7562, 0.5619, 0.8856, 0.8117, 0.6927, 0.7509, 0.8527, 0.8272, 0.8098, 0.8942, 0.8724] 
2025-02-25 05:18:13.137293: Epoch time: 391.94 s 
2025-02-25 05:18:13.141739: Yayy! New best EMA pseudo Dice: 0.804 
2025-02-25 05:18:17.710131:  
2025-02-25 05:18:17.715632: Epoch 58 
2025-02-25 05:18:17.719913: Current learning rate: 0.00948 
2025-02-25 05:24:54.653642: train_loss -0.8345 
2025-02-25 05:24:54.660644: val_loss -0.5698 
2025-02-25 05:24:54.667538: Pseudo dice [0.9491, 0.7554, 0.5738, 0.8788, 0.7971, 0.6843, 0.7578, 0.8509, 0.824, 0.8139, 0.8974, 0.8765] 
2025-02-25 05:24:54.673968: Epoch time: 396.95 s 
2025-02-25 05:24:54.680560: Yayy! New best EMA pseudo Dice: 0.8041 
2025-02-25 05:24:59.236165:  
2025-02-25 05:24:59.243453: Epoch 59 
2025-02-25 05:24:59.251151: Current learning rate: 0.00947 
